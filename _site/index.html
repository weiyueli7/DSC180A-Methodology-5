<!doctype html>
<!-- The Time Machine GitHub pages theme was designed and developed by Jon Rohan, on Feb 7, 2012. -->
<!-- Follow him for fun. http://twitter.com/jonrohan. Tail his code on https://github.com/jonrohan -->
<html lang="en-US">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width,initial-scale=1">

  <link rel="stylesheet" href="/assets/css/style.css?v=f8235816271c6ea953ae8b530270ebef1d63fa0a">
  <script src="https://code.jquery.com/jquery-1.12.4.min.js" integrity="sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=" crossorigin="anonymous"></script>
  <script src="/assets/js/script.js"></script>

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>DSC180A-Methodology-5</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="DSC180A-Methodology-5" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/" />
<meta property="og:url" content="http://localhost:4000/" />
<meta property="og:site_name" content="DSC180A-Methodology-5" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="DSC180A-Methodology-5" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","headline":"DSC180A-Methodology-5","name":"DSC180A-Methodology-5","url":"http://localhost:4000/"}</script>
<!-- End Jekyll SEO tag -->


  <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" -->

<!-- end custom head snippets -->

</head>

<body>

  <div class="wrapper">
    <header>
      <h1 class="title">DSC180A-Methodology-5</h1>
    </header>
    <div id="container">
      <p class="tagline"></p>
      <div id="main" role="main">
        <div class="download-bar">
        <div class="inner">
          
          <a href="https://github.com/weiyueli7/DSC180A-Methodology-5" class="code">View DSC180A-Methodology-5 on GitHub</a>
        </div>
        <span class="blc"></span><span class="trc"></span>
        </div>
        <article class="markdown-body">
          <p>Name: <a href="https://weiyueli7.github.io">Weiyue Li</a>, Email: <a href="mailto:wel019@ucsd.edu">wel019@ucsd.edu</a></p>

<p>Section: <a href="https://dsc-capstone.org/enrollment/#%EF%B8%8F-language-models">B17</a>, Mentor: Professor <a href="https://cseweb.ucsd.edu/~haozhang/">Hao Zhang</a></p>

<ol>
  <li><strong>What is the most interesting topic covered in your domain this quarter?</strong>
    <ul>
      <li>The most interesting topic covered in my domain this quarter is the memory and flops required by training and serving large language models. I found it interesting because I didn’t really have this sort of background and most of the time when I encounter the cuda out of memory error I could only reduce the batch size and hope for the best. I think it’s really cool to know that there are other ways to reduce the memory usage and I’m excited to try them out in the future.</li>
    </ul>
  </li>
  <li><strong>Describe a potential investigation you would like to pursue for your Quarter 2 Project.</strong>
    <ul>
      <li>For my Quarter 2 Project, I would like to investigate the efficiency of different model compression techniques on large language models. This could involve exploring methods like quantization, pruning, and knowledge distillation to see how they affect model performance and memory requirements. I’m particularly interested in finding a balance between model size, speed, and accuracy, which could be highly beneficial for deploying these models in resource-constrained environments.</li>
    </ul>
  </li>
  <li><strong>What is a potential change you’d make to the approach taken in your current Quarter 1 Project?</strong>
    <ul>
      <li>In retrospect, a potential change for my Quarter 1 Project would be to incorporate a more thorough evaluation of memory usage across different stages of model training and deployment. This could include profiling memory usage during different training phases and experimenting with various batch sizes or model architectures to optimize memory consumption without significantly impacting model performance.</li>
    </ul>
  </li>
  <li><strong>What other techniques would you be interested in using in your project?</strong>
    <ul>
      <li>In addition to memory optimization techniques, I’m interested in exploring parallel computing frameworks such as PyTorch with multi-GPU support or distributed training methods to enhance training efficiency. Also, implementing automated machine learning (AutoML) techniques to find optimal model architectures could be another exciting area to explore, potentially leading to more efficient models that require less computational power.</li>
    </ul>
  </li>
</ol>

        </article>
      </div>
    </div>
    <footer>
      <div class="owner">
      
      <p><a href="https://github.com/weiyueli7" class="avatar"><img src="https://github.com/weiyueli7.png" width="48" height="48"></a> <a href="https://github.com/weiyueli7">weiyueli7</a> maintains <a href="https://github.com/weiyueli7/DSC180A-Methodology-5">DSC180A-Methodology-5</a></p>
      

       

      </div>
      <div class="creds">
        <small>This page generated using <a href="https://pages.github.com/">GitHub Pages</a><br>theme by <a href="https://twitter.com/jonrohan/">Jon Rohan</a></small>
      </div>
    </footer>
  </div>
  <div class="current-section">
    <a href="#top">Scroll to top</a>
    
    <p class="name"></p>
  </div>
</body>
</html>
