Name: [Weiyue Li](https://weiyueli7.github.io), Email: [wel019@ucsd.edu](mailto:wel019@ucsd.edu)

Section: [B17](https://dsc-capstone.org/enrollment/#%EF%B8%8F-language-models), Mentor: Professor [Hao Zhang](https://cseweb.ucsd.edu/~haozhang/)

1. **What is the most interesting topic covered in your domain this quarter?**
   - The most interesting topic covered in my domain this quarter is the memory and flops required by training and serving large language models. I found it interesting because I didn't really have this sort of background and most of the time when I encounter the cuda out of memory error I could only reduce the batch size and hope for the best. I think it's really cool to know that there are other ways to reduce the memory usage and I'm excited to try them out in the future.

2. **Describe a potential investigation you would like to pursue for your Quarter 2 Project.**
   - For my Quarter 2 Project, I would like to investigate the efficiency of different model compression techniques on large language models. This could involve exploring methods like quantization, pruning, and knowledge distillation to see how they affect model performance and memory requirements. I'm particularly interested in finding a balance between model size, speed, and accuracy, which could be highly beneficial for deploying these models in resource-constrained environments.

3. **What is a potential change youâ€™d make to the approach taken in your current Quarter 1 Project?**
   - In retrospect, a potential change for my Quarter 1 Project would be to incorporate a more thorough evaluation of memory usage across different stages of model training and deployment. This could include profiling memory usage during different training phases and experimenting with various batch sizes or model architectures to optimize memory consumption without significantly impacting model performance.

4. **What other techniques would you be interested in using in your project?**
   - In addition to memory optimization techniques, I'm interested in exploring parallel computing frameworks such as PyTorch with multi-GPU support or distributed training methods to enhance training efficiency. Also, implementing automated machine learning (AutoML) techniques to find optimal model architectures could be another exciting area to explore, potentially leading to more efficient models that require less computational power.
